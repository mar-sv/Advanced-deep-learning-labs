{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to use Flickr8-dataset. The dataset is available on kaggle here: https://www.kaggle.com/datasets/adityajn105/flickr8k?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 14:28:20.586566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Applied AI\\\\Advanced deep learning\\\\Advanced_deep_learning\\\\Advanced-deep-learning-labs\\\\lab_3\\\\Flickr8_data\\\\captions.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         captions[image_id]\u001b[38;5;241m.\u001b[39mappend(image_desc)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m captions\n\u001b[0;32m---> 28\u001b[0m captions \u001b[38;5;241m=\u001b[39m load_captions(caption_file)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_images\u001b[39m(image_folder, captions):\n",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m, in \u001b[0;36mload_captions\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_captions\u001b[39m(filename):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     16\u001b[0m         text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     17\u001b[0m     captions \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Applied AI\\\\Advanced deep learning\\\\Advanced_deep_learning\\\\Advanced-deep-learning-labs\\\\lab_3\\\\Flickr8_data\\\\captions.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Path to the Flickr8k images and captions\n",
    "image_folder = r\"C:\\Applied AI\\Advanced deep learning\\Advanced_deep_learning\\Advanced-deep-learning-labs\\lab_3\\Flickr8_data\\Images\" #CHANGE HERE TO YOUR OWN PATH!\n",
    "caption_file = r\"C:\\Applied AI\\Advanced deep learning\\Advanced_deep_learning\\Advanced-deep-learning-labs\\lab_3\\Flickr8_data\\captions.txt\" #SAME HERE!\n",
    "\n",
    "# Load captions\n",
    "def load_captions(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    captions = {}\n",
    "    for line in text.strip().split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in captions:\n",
    "            captions[image_id] = []\n",
    "        captions[image_id].append(image_desc)\n",
    "    return captions\n",
    "\n",
    "captions = load_captions(caption_file)\n",
    "\n",
    "# Load images\n",
    "def load_images(image_folder, captions):\n",
    "    images = {}\n",
    "    for image_id in captions:\n",
    "        image_path = os.path.join(image_folder, image_id + '.jpg')\n",
    "        image = Image.open(image_path)\n",
    "        images[image_id] = np.array(image)\n",
    "    return images\n",
    "\n",
    "images = load_images(image_folder, captions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    images_processed = {}\n",
    "    for image_id, image in images.items():\n",
    "        image = Image.fromarray(image).resize((224, 224))  # Resize images\n",
    "        image = np.array(image)\n",
    "        image = preprocess_input(image)  # Normalize images for VGG16\n",
    "        images_processed[image_id] = image\n",
    "    return images_processed\n",
    "\n",
    "images_preprocessed = preprocess_images(images)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<unk>\")\n",
    "all_captions = [caption for caption_list in captions.values() for caption in caption_list]\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "captions_seq = {image_id: tokenizer.texts_to_sequences(captions_list) for image_id, captions_list in captions.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Here we are going to create a model. The model has an input shape of 224,224, 3, and will use the weights from imagenet.**\n",
    "\n",
    "#### **This model is then connected to a LSTM, for image captioning.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def create_model(vocab_size, max_length):\n",
    "    \n",
    "    vgg = VGG16(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "    vgg.trainable = False  \n",
    "    image_input = Input(shape=(224, 224, 3))\n",
    "    features = vgg(image_input)\n",
    "    features = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
    "    image_dense = Dense(256, activation='relu')(features)\n",
    "    \n",
    "    # Caption model\n",
    "    caption_input = Input(shape=(max_length,))\n",
    "    caption_embedding = Embedding(input_dim=vocab_size, output_dim=256)(caption_input)\n",
    "    caption_lstm = LSTM(256)(caption_embedding)\n",
    "    \n",
    "    # Combine image and caption information\n",
    "    decoder_input = Concatenate()([image_dense, caption_lstm])\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder_input)\n",
    "    \n",
    "    # Final model\n",
    "    model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
