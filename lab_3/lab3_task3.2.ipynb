{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to use Flickr8-dataset. The dataset is available on kaggle here: https://www.kaggle.com/datasets/adityajn105/flickr8k?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Path to the Flickr8k images and captions\n",
    "image_folder = r\"C:\\Applied AI\\Advanced deep learning\\Advanced_deep_learning\\Advanced-deep-learning-labs\\lab_3\\Flickr8_data\\Images\" #CHANGE HERE TO YOUR OWN PATH!\n",
    "caption_file = r\"C:\\Applied AI\\Advanced deep learning\\Advanced_deep_learning\\Advanced-deep-learning-labs\\lab_3\\Flickr8_data\\captions.txt\" #SAME HERE!\n",
    "\n",
    "# Load captions\n",
    "def load_captions(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    captions = {}\n",
    "    for line in text.strip().split('\\n')[1:]:\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in captions:\n",
    "            captions[image_id] = []\n",
    "        captions[image_id].append(image_desc)\n",
    "    return captions\n",
    "\n",
    "captions = load_captions(caption_file)\n",
    "\n",
    "# Load images\n",
    "def load_images(image_folder, captions):\n",
    "    images = {}\n",
    "    for image_id in captions:\n",
    "        image_path = os.path.join(image_folder, image_id + '.jpg')\n",
    "        image = Image.open(image_path)\n",
    "        images[image_id] = np.array(image)\n",
    "    return images\n",
    "\n",
    "images = load_images(image_folder, captions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    images_processed = {}\n",
    "    for image_id, image in images.items():\n",
    "        image = Image.fromarray(image).resize((224, 224))  # Resize images\n",
    "        image = np.array(image)\n",
    "        image = preprocess_input(image)  # Normalize images for VGG16\n",
    "        images_processed[image_id] = image\n",
    "    return images_processed\n",
    "\n",
    "images_preprocessed = preprocess_images(images)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<unk>\")\n",
    "all_captions = [caption for caption_list in captions.values() for caption in caption_list]\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "captions_seq = {image_id: tokenizer.texts_to_sequences(captions_list) for image_id, captions_list in captions.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Here we are going to create a model. The model has an input shape of 224,224, 3, and will use the weights from imagenet.**\n",
    "\n",
    "#### **This model is then connected to a LSTM, for image captioning.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def create_model(vocab_size, max_length):\n",
    "    \n",
    "    vgg = VGG16(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "    vgg.trainable = False  \n",
    "    image_input = Input(shape=(224, 224, 3))\n",
    "    features = vgg(image_input)\n",
    "    features = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
    "    image_dense = Dense(256, activation='relu')(features)\n",
    "    \n",
    "    # Caption model\n",
    "    caption_input = Input(shape=(max_length,))\n",
    "    caption_embedding = Embedding(input_dim=vocab_size, output_dim=256)(caption_input)\n",
    "    caption_lstm = LSTM(256)(caption_embedding)\n",
    "    \n",
    "    # Combine image and caption information\n",
    "    decoder_input = Concatenate()([image_dense, caption_lstm])\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder_input)\n",
    "    \n",
    "    # Final model\n",
    "    model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
